{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11a7768",
   "metadata": {},
   "source": [
    "# IIT 4316 Deep Learning<br>Homework #2-1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926e2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "\n",
    "DIGITS     = ['0','1','2','3','4','5','6','7','8','9']\n",
    "VOCAB_SIZE = len(DIGITS)\n",
    "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Fixed hyperparameters\n",
    "INPUT_LEN     = 4\n",
    "OUTPUT_LEN    = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCH     = 1000\n",
    "BATCH_SIZE    = 128\n",
    "\n",
    "# Changable hyperparameters\n",
    "EMBED_DIM      = 16\n",
    "CONV_LAYER_NUM = 2\n",
    "NUM_CHANNELS   = 32\n",
    "\n",
    "char_to_idx = {c: i for i, c in enumerate(DIGITS)}\n",
    "idx_to_char = {i: c for i, c in enumerate(DIGITS)}\n",
    "\n",
    "# batch data generation\n",
    "def generate_batch(batch_size=100):\n",
    "    inputs, targets = [], []\n",
    "    for _ in range(batch_size):\n",
    "        tens1, ones1 = random.randint(0, 9), random.randint(0, 9)\n",
    "        tens2, ones2 = random.randint(0, 9), random.randint(0, 9)\n",
    "        num1, num2 = tens1 * 10 + ones1, tens2 * 10 + ones2\n",
    "        s = num1 + num2\n",
    "        inputs.append([tens1, ones1, tens2, ones2])\n",
    "        sum_str = f\"{s:03d}\"\n",
    "        targets.append([char_to_idx[c] for c in sum_str])\n",
    "    return (torch.tensor(inputs, dtype=torch.long).to(DEVICE),\n",
    "            torch.tensor(targets, dtype=torch.long).to(DEVICE))\n",
    "\n",
    "# ReLU\n",
    "def my_relu(x):\n",
    "    return torch.clamp(x, min=0.0)\n",
    "\n",
    "#------------------------------------\n",
    "# 4 digits to one-hot vectors\n",
    "#------------------------------------\n",
    "def MyOneHot(x, vocab_size):    \n",
    "    B, L = x.size()     # batch size x INPUT_LEN\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Convert x to one-hot\n",
    "    ############################################################################\n",
    "\n",
    "    # Create zero tensor of shape (B, L, vocab_size)\n",
    "    out = torch.zeros(B, L, vocab_size, device=x.device)\n",
    "    # Use scatter to fill in the one-hot encoding\n",
    "    out.scatter_(2, x.unsqueeze(2), 1.0)\n",
    "\n",
    "    ############################################################################\n",
    "    # END TODO\n",
    "    ############################################################################\n",
    "\n",
    "    return out   # B x L x vocab_size\n",
    "\n",
    "#------------------------------------\n",
    "# Embedding layer\n",
    "#------------------------------------\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(vocab_size, dim) * 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Convert x to one-hot and then embedding\n",
    "        #    Use MyOneHot() function above\n",
    "        ############################################################################\n",
    "\n",
    "        # Convert to one-hot: (B, L, vocab_size)\n",
    "        one_hot = MyOneHot(x, self.weight.size(0))\n",
    "        # Matrix multiply with embedding weight: (B, L, vocab_size) @ (vocab_size, dim) = (B, L, dim)\n",
    "        out = torch.matmul(one_hot, self.weight)\n",
    "        \n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out  # B x L x dim\n",
    "\n",
    "#------------------------------------\n",
    "# Linear layer\n",
    "#------------------------------------\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim) * 0.1)\n",
    "        self.bias   = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Compute linear layer output\n",
    "        #   You should implement your own linear layer operation.\n",
    "        ############################################################################\n",
    "            \n",
    "        # x: (..., in_dim), weight: (out_dim, in_dim), bias: (out_dim)\n",
    "        # out = x @ W^T + bias\n",
    "        out = torch.matmul(x, self.weight.t()) + self.bias\n",
    "        \n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out  \n",
    "\n",
    "#------------------------------------\n",
    "# Conv layer\n",
    "#------------------------------------\n",
    "class MyConv2D(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channel  = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernel      = kernel_size\n",
    "        self.pad         = padding\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(out_channel, in_channel, kernel_size, kernel_size) * 0.1)\n",
    "        self.bias   = nn.Parameter(torch.zeros(out_channel))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: 2D convolution operation\n",
    "        #   You should implement your own convolution operation.\n",
    "        ############################################################################\n",
    "\n",
    "        # Pad the input\n",
    "        if self.pad > 0:\n",
    "            x_padded = torch.zeros(B, C, H + 2 * self.pad, W + 2 * self.pad, device=x.device)\n",
    "            x_padded[:, :, self.pad:self.pad + H, self.pad:self.pad + W] = x\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        H_pad, W_pad = x_padded.shape[2], x_padded.shape[3]\n",
    "        \n",
    "        # Output dimensions (stride=1)\n",
    "        H_out = H_pad - self.kernel + 1\n",
    "        W_out = W_pad - self.kernel + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        out = torch.zeros(B, self.out_channel, H_out, W_out, device=x.device)\n",
    "        \n",
    "        # Perform convolution\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                # Extract patch: (B, C, kernel, kernel)\n",
    "                patch = x_padded[:, :, i:i + self.kernel, j:j + self.kernel]\n",
    "                # patch: (B, in_channel, kernel, kernel)\n",
    "                # weight: (out_channel, in_channel, kernel, kernel)\n",
    "                # Compute convolution for all output channels at once\n",
    "                for oc in range(self.out_channel):\n",
    "                    # Sum over in_channel, kernel_h, kernel_w\n",
    "                    out[:, oc, i, j] = (patch * self.weight[oc]).sum(dim=(1, 2, 3)) + self.bias[oc]\n",
    "\n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out  # B x out_ch x H x W\n",
    "\n",
    "#------------------------------------\n",
    "# CNN model\n",
    "#------------------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_channels, num_conv_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Define each layer (embedding, conv1, conv2, ..., fc)\n",
    "        #     Use MyEmbedding, MyConv2D, MyLinear classes defined above.\n",
    "        ############################################################################\n",
    "        \n",
    "        self.embed_dim       = embed_dim\n",
    "        self.num_channels    = num_channels\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = MyEmbedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv layers using nn.ModuleList for dynamic number of layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            if i == 0:\n",
    "                # First conv: in_channel = embed_dim, out_channel = num_channels\n",
    "                self.conv_layers.append(MyConv2D(embed_dim, num_channels, kernel_size=3, padding=1))\n",
    "            else:\n",
    "                # Subsequent convs: in_channel = num_channels, out_channel = num_channels\n",
    "                self.conv_layers.append(MyConv2D(num_channels, num_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        # FC layer: input is flattened conv output (2 * 2 * num_channels), output is 3 * VOCAB_SIZE\n",
    "        self.fc = MyLinear(2 * 2 * num_channels, OUTPUT_LEN * vocab_size)\n",
    "\n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Define forward path: \n",
    "        #    embedding -> reshape -> conv1 -> ReLU -> conv2 -> ReLU -> ... -> flatten -> fc\n",
    "        ############################################################################\n",
    "\n",
    "        # Embedding: (B, 4) -> (B, 4, embed_dim)\n",
    "        out = self.embedding(x)\n",
    "\n",
    "        # Reshape to image-like: (B, 4, embed_dim) -> (B, embed_dim, 2, 2)\n",
    "        out = out.permute(0, 2, 1)  # (B, embed_dim, 4)\n",
    "        out = out.view(B, self.embed_dim, 2, 2)  # (B, embed_dim, 2, 2)\n",
    "        \n",
    "        # Apply conv layers with ReLU\n",
    "        for conv in self.conv_layers:\n",
    "            out = conv(out)  # (B, num_channels, 2, 2)\n",
    "            out = my_relu(out)\n",
    "        \n",
    "        # Flatten\n",
    "        out = out.view(B, -1)  # (B, 2*2*num_channels)\n",
    "        \n",
    "        # FC layer\n",
    "        out = self.fc(out)  # (B, 3*VOCAB_SIZE)\n",
    "        \n",
    "        # Reshape to (B, 3, VOCAB_SIZE)\n",
    "        out = out.view(B, OUTPUT_LEN, VOCAB_SIZE)\n",
    "\n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out    # B x 3 x VOCAB_SIZE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Training & test\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    model     = MyCNN(VOCAB_SIZE, EMBED_DIM, NUM_CHANNELS, CONV_LAYER_NUM).to(DEVICE)\n",
    "    optim     = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in tq(range(1, NUM_EPOCH+1)):   \n",
    "        model.train()\n",
    "        src, tgt = generate_batch(BATCH_SIZE)\n",
    "        logits   = model(src)\n",
    "        loss     = criterion(logits.view(-1, VOCAB_SIZE), tgt.view(-1))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d}  Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    with torch.no_grad():\n",
    "        src, tgt = generate_batch(100)\n",
    "        logits   = model(src)\n",
    "        preds    = logits.argmax(-1)\n",
    "        for s, t, p in zip(src.cpu().tolist(), tgt.cpu().tolist(), preds.cpu().tolist()):\n",
    "            s_str = f\"{s[0]}{s[1]} + {s[2]}{s[3]}\"\n",
    "            t_str = ''.join(idx_to_char[x] for x in t)\n",
    "            p_str = ''.join(idx_to_char[x] for x in p)\n",
    "            print(f\"src: {s_str} = {t_str} | pred: {p_str}\")\n",
    "            if t_str == p_str:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(f\"Correct: {correct}/{total}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69103779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Experiment 1: Varying EMBED_DIM\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  12%|█▎        | 1/8 [00:45<05:20, 45.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM= 4 | Loss: 1.6839 | Correct: 4/1000 | Time: 45.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  25%|██▌       | 2/8 [01:29<04:28, 44.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM= 8 | Loss: 1.5850 | Correct: 21/1000 | Time: 43.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  38%|███▊      | 3/8 [02:13<03:41, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM=12 | Loss: 1.5841 | Correct: 19/1000 | Time: 43.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  50%|█████     | 4/8 [03:00<03:01, 45.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM=16 | Loss: 1.5736 | Correct: 14/1000 | Time: 46.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  62%|██████▎   | 5/8 [03:48<02:19, 46.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM=24 | Loss: 1.5421 | Correct: 30/1000 | Time: 48.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  75%|███████▌  | 6/8 [04:39<01:35, 47.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM=32 | Loss: 1.5344 | Correct: 38/1000 | Time: 50.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM:  88%|████████▊ | 7/8 [05:52<00:56, 56.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM=48 | Loss: 1.5251 | Correct: 43/1000 | Time: 73.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMBED_DIM: 100%|██████████| 8/8 [06:39<00:00, 49.97s/it]\n",
      "EMBED_DIM: 100%|██████████| 8/8 [06:39<00:00, 49.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM=64 | Loss: 1.4730 | Correct: 65/1000 | Time: 46.74s\n",
      "\n",
      "==================================================\n",
      "Experiment 2: Varying CONV_LAYER_NUM\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM:  12%|█▎        | 1/8 [00:28<03:17, 28.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM=1 | Loss: 1.7076 | Correct: 10/1000 | Time: 28.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM:  25%|██▌       | 2/8 [01:17<04:03, 40.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM=2 | Loss: 1.5736 | Correct: 14/1000 | Time: 49.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM:  38%|███▊      | 3/8 [02:24<04:24, 52.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM=3 | Loss: 1.5055 | Correct: 40/1000 | Time: 67.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM:  50%|█████     | 4/8 [03:52<04:27, 66.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM=4 | Loss: 1.2434 | Correct: 80/1000 | Time: 88.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM:  62%|██████▎   | 5/8 [05:58<04:23, 87.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV_LAYER_NUM=5 | Loss: 1.1295 | Correct: 84/1000 | Time: 125.08s\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameter Experiment for CNN\n",
    "# ============================================================\n",
    "\n",
    "def train_and_evaluate(vocab_size, embed_dim, num_channels, num_conv_layers, \n",
    "                       num_epochs=1000, batch_size=128, lr=1e-4, test_size=1000):\n",
    "    \"\"\"Train model and return final loss, test accuracy, and training time.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    model     = MyCNN(vocab_size, embed_dim, num_channels, num_conv_layers).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training with time measurement\n",
    "    start_time = time.time()\n",
    "    final_loss = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        src, tgt = generate_batch(batch_size)\n",
    "        logits   = model(src)\n",
    "        loss     = criterion(logits.view(-1, VOCAB_SIZE), tgt.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss = loss.item()\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        src, tgt = generate_batch(test_size)\n",
    "        logits   = model(src)\n",
    "        preds    = logits.argmax(-1)\n",
    "        for t, p in zip(tgt.cpu().tolist(), preds.cpu().tolist()):\n",
    "            if t == p:\n",
    "                correct += 1\n",
    "    \n",
    "    return final_loss, correct, train_time\n",
    "\n",
    "# Default values\n",
    "DEFAULT_EMBED_DIM      = 16\n",
    "DEFAULT_CONV_LAYER_NUM = 2\n",
    "DEFAULT_NUM_CHANNELS   = 32\n",
    "\n",
    "# ============================================================\n",
    "# Experiment 1: Varying EMBED_DIM\n",
    "# ============================================================\n",
    "print(\"=\" * 50)\n",
    "print(\"Experiment 1: Varying EMBED_DIM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "embed_dims   = [4, 8, 12, 16, 24, 32, 48, 64]\n",
    "\n",
    "losses_embed = []\n",
    "accs_embed   = []\n",
    "times_embed  = []\n",
    "\n",
    "for ed in tq(embed_dims, desc=\"EMBED_DIM\"):\n",
    "    loss, acc, t = train_and_evaluate(VOCAB_SIZE, ed, DEFAULT_NUM_CHANNELS, DEFAULT_CONV_LAYER_NUM)\n",
    "    losses_embed.append(loss)\n",
    "    accs_embed.append(acc)\n",
    "    times_embed.append(t)\n",
    "    print(f\"EMBED_DIM={ed:2d} | Loss: {loss:.4f} | Correct: {acc}/1000 | Time: {t:.2f}s\")\n",
    "\n",
    "# ============================================================\n",
    "# Experiment 2: Varying CONV_LAYER_NUM\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Experiment 2: Varying CONV_LAYER_NUM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "conv_layers = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "losses_conv = []\n",
    "accs_conv   = []\n",
    "times_conv  = []\n",
    "\n",
    "for nl in tq(conv_layers, desc=\"CONV_LAYER_NUM\"):\n",
    "    loss, acc, t = train_and_evaluate(VOCAB_SIZE, DEFAULT_EMBED_DIM, DEFAULT_NUM_CHANNELS, nl)\n",
    "    losses_conv.append(loss)\n",
    "    accs_conv.append(acc)\n",
    "    times_conv.append(t)\n",
    "    print(f\"CONV_LAYER_NUM={nl} | Loss: {loss:.4f} | Correct: {acc}/1000 | Time: {t:.2f}s\")\n",
    "\n",
    "# ============================================================\n",
    "# Experiment 3: Varying NUM_CHANNELS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Experiment 3: Varying NUM_CHANNELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_channels_list = [8, 16, 24, 32, 48, 64, 96, 128]\n",
    "losses_channels   = []\n",
    "accs_channels     = []\n",
    "times_channels    = []\n",
    "\n",
    "for nc in tq(num_channels_list, desc=\"NUM_CHANNELS\"):\n",
    "    loss, acc, t = train_and_evaluate(VOCAB_SIZE, DEFAULT_EMBED_DIM, nc, DEFAULT_CONV_LAYER_NUM)\n",
    "    losses_channels.append(loss)\n",
    "    accs_channels.append(acc)\n",
    "    times_channels.append(t)\n",
    "    print(f\"NUM_CHANNELS={nc:3d} | Loss: {loss:.4f} | Correct: {acc}/1000 | Time: {t:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31243b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Plotting - Single Figure with 3 y-axes\n",
    "# ============================================================\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Create additional y-axes\n",
    "ax2 = ax1.twinx()\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))\n",
    "\n",
    "# X-axis: use indices (0 to n-1)\n",
    "n = len(embed_dims)\n",
    "x_indices = list(range(n))\n",
    "\n",
    "# Plot Train Loss (RED) - Left y-axis\n",
    "# Solid: EMBED_DIM, Dotted: CONV_LAYER_NUM, Dashed: NUM_CHANNELS\n",
    "l1 = ax1.plot(x_indices, losses_embed, 'r-o', linewidth=2, markersize=6, label='EMBED_DIM (Loss)')\n",
    "l2 = ax1.plot(x_indices, losses_conv, 'r:s', linewidth=2, markersize=6, label='CONV_LAYER_NUM (Loss)')\n",
    "l3 = ax1.plot(x_indices, losses_channels, 'r--^', linewidth=2, markersize=6, label='NUM_CHANNELS (Loss)')\n",
    "\n",
    "# Plot Test Accuracy (GREEN) - Right y-axis 1\n",
    "l4 = ax2.plot(x_indices, accs_embed, 'g-o', linewidth=2, markersize=6, label='EMBED_DIM (Acc)')\n",
    "l5 = ax2.plot(x_indices, accs_conv, 'g:s', linewidth=2, markersize=6, label='CONV_LAYER_NUM (Acc)')\n",
    "l6 = ax2.plot(x_indices, accs_channels, 'g--^', linewidth=2, markersize=6, label='NUM_CHANNELS (Acc)')\n",
    "\n",
    "# Plot Training Time (BLUE) - Right y-axis 2\n",
    "l7 = ax3.plot(x_indices, times_embed, 'b-o', linewidth=2, markersize=6, alpha=0.7, label='EMBED_DIM (Time)')\n",
    "l8 = ax3.plot(x_indices, times_conv, 'b:s', linewidth=2, markersize=6, alpha=0.7, label='CONV_LAYER_NUM (Time)')\n",
    "l9 = ax3.plot(x_indices, times_channels, 'b--^', linewidth=2, markersize=6, alpha=0.7, label='NUM_CHANNELS (Time)')\n",
    "\n",
    "# Labels\n",
    "ax1.set_xlabel('Index (see legend for actual values)', fontsize=12)\n",
    "ax1.set_ylabel('Train Loss (RED)', fontsize=12, color='red')\n",
    "ax2.set_ylabel('Test Correct /1000 (GREEN)', fontsize=12, color='green')\n",
    "ax3.set_ylabel('Training Time in sec (BLUE)', fontsize=12, color='blue')\n",
    "\n",
    "# Set x-axis ticks\n",
    "ax1.set_xticks(x_indices)\n",
    "ax1.set_xticklabels([f'{i}' for i in x_indices])\n",
    "\n",
    "# Add text annotation for actual values\n",
    "embed_str   = f\"EMBED_DIM: {embed_dims}\"\n",
    "conv_str    = f\"CONV_LAYER_NUM: {conv_layers}\"\n",
    "channel_str = f\"NUM_CHANNELS: {num_channels_list}\"\n",
    "fig.text(0.5, -0.02, f\"{embed_str}\\n{conv_str}\\n{channel_str}\", \n",
    "         ha='center', fontsize=9, family='monospace')\n",
    "\n",
    "# Combine legends\n",
    "lines  = l1 + l2 + l3 + l4 + l5 + l6 + l7 + l8 + l9\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper left', fontsize=8, ncol=3)\n",
    "\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.title('CNN Hyperparameter Analysis\\n(Color: Metric, Style: Hyperparameter | Solid: EMBED, Dotted: LAYER, Dashed: CHANNEL)', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede8035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2 (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

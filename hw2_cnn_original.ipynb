{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11a7768",
   "metadata": {},
   "source": [
    "# IIT 4316 Deep Learning<br>Homework #2-1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "\n",
    "DIGITS = ['0','1','2','3','4','5','6','7','8','9']\n",
    "VOCAB_SIZE = len(DIGITS)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "INPUT_LEN = 4\n",
    "OUTPUT_LEN = 3\n",
    "EMBED_DIM = \n",
    "NUM_CHANNELS = \n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCH = 1000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "char_to_idx = {c: i for i, c in enumerate(DIGITS)}\n",
    "idx_to_char = {i: c for i, c in enumerate(DIGITS)}\n",
    "\n",
    "# batch data generation\n",
    "def generate_batch(batch_size=100):\n",
    "    inputs, targets = [], []\n",
    "    for _ in range(batch_size):\n",
    "        tens1, ones1 = random.randint(0, 9), random.randint(0, 9)\n",
    "        tens2, ones2 = random.randint(0, 9), random.randint(0, 9)\n",
    "        num1, num2 = tens1 * 10 + ones1, tens2 * 10 + ones2\n",
    "        s = num1 + num2\n",
    "        inputs.append([tens1, ones1, tens2, ones2])\n",
    "        sum_str = f\"{s:03d}\"\n",
    "        targets.append([char_to_idx[c] for c in sum_str])\n",
    "    return (torch.tensor(inputs, dtype=torch.long).to(DEVICE),\n",
    "            torch.tensor(targets, dtype=torch.long).to(DEVICE))\n",
    "\n",
    "# ReLU\n",
    "def my_relu(x):\n",
    "    return torch.clamp(x, min=0.0)\n",
    "\n",
    "#------------------------------------\n",
    "# 4 digits to one-hot vectors\n",
    "#------------------------------------\n",
    "def MyOneHot(x, vocab_size):    \n",
    "    B, L = x.size()     # batch size x INPUT_LEN\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Convert x to one-hot\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # END TODO\n",
    "    ############################################################################\n",
    "\n",
    "    return out   # B x L x vocab_size\n",
    "\n",
    "#------------------------------------\n",
    "# Embedding layer\n",
    "#------------------------------------\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(vocab_size, dim) * 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Convert x to one-hot and then embedding\n",
    "        #    Use MyOneHot() function above\n",
    "        ############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out  # B x L x dim\n",
    "\n",
    "#------------------------------------\n",
    "# Linear layer\n",
    "#------------------------------------\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Compute linear layer output\n",
    "        #   You should implement your own linear layer operation.\n",
    "        ############################################################################\n",
    "            \n",
    "      \n",
    "        \n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out  \n",
    "\n",
    "#------------------------------------\n",
    "# Conv layer\n",
    "#------------------------------------\n",
    "class MyConv2D(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernel = kernel_size\n",
    "        self.pad = padding\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(out_channel, in_channel, kernel_size, kernel_size) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channel))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: 2D convolution operation\n",
    "        #   You should implement your own convolution operation.\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out  # B x out_ch x H x W\n",
    "\n",
    "#------------------------------------\n",
    "# CNN model\n",
    "#------------------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Define each layer (embedding, conv1, conv2, ..., fc)\n",
    "        #     Use MyEmbedding, MyConv2D, MyLinear classes defined above.\n",
    "        ############################################################################\n",
    "                \n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Define forward path: \n",
    "        #    embedding -> reshape -> conv1 -> ReLU -> conv2 -> ReLU -> ... -> flatten -> fc\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "        # END TODO\n",
    "        ############################################################################\n",
    "\n",
    "        return out    # B x 3 x VOCAB_SIZE\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Training & test\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    model = MyCNN(VOCAB_SIZE, EMBED_DIM, NUM_CHANNELS).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCH+1):   \n",
    "        model.train()\n",
    "        src, tgt = generate_batch(BATCH_SIZE)\n",
    "        logits = model(src)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), tgt.view(-1))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}  Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        src, tgt = generate_batch(100)\n",
    "        logits = model(src)\n",
    "        preds = logits.argmax(-1)\n",
    "        for s, t, p in zip(src.cpu().tolist(), tgt.cpu().tolist(), preds.cpu().tolist()):\n",
    "            s_str = f\"{s[0]}{s[1]} + {s[2]}{s[3]}\"\n",
    "            t_str = ''.join(idx_to_char[x] for x in t)\n",
    "            p_str = ''.join(idx_to_char[x] for x in p)\n",
    "            print(f\"src: {s_str} = {t_str} | pred: {p_str}\")\n",
    "            if t_str == p_str:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(f\"Correct: {correct}/{total}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (VS Code)",
   "language": "python",
   "name": "python3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
